import numpy as np
import argparse
import glob
import os
import re
from functools import partial
import vispy

from tqdm import tqdm
import yaml
import time
import sys
from mesh import write_ply, read_ply, output_3d_photo
from utils import get_MiDaS_samples, read_MiDaS_depth
import torch
import cv2
from skimage.transform import resize
import imageio
import copy
from networks import Inpaint_Color_Net, Inpaint_Depth_Net, Inpaint_Edge_Net
from DA2_depth import run_depth_anything_v2
"""
Note on optional depth backends:
- MiDaS and BoostMonoDepth are optional and imported lazily only when enabled in config.
- This allows removing the `MiDaS/` directory when not used.
"""
from bilateral_filtering import sparse_bilateral_filtering

parser = argparse.ArgumentParser()
parser.add_argument('--config', type=str, default='argument.yml',help='Configure of post processing')
args = parser.parse_args()
config = yaml.load(open(args.config, 'r'), Loader=yaml.SafeLoader)
# Debug outputs
DEBUG_MODE = os.environ.get('DEBUG_MODE') is not None
DEBUG_DIR = os.environ.get('DEBUG_DIR') or None

# Optional runtime overrides
env_specific = os.environ.get('SPECIFIC_NAME')
if env_specific:
    config['specific'] = env_specific
env_longer_side = os.environ.get('LONGER_SIDE_LEN')
if env_longer_side:
    try:
        config['longer_side_len'] = int(env_longer_side)
    except Exception:
        pass
env_save_ply = os.environ.get('SAVE_PLY')
if env_save_ply is not None:
    config['save_ply'] = str(env_save_ply).lower() in ('1', 'true', 'yes', 'y')
env_encoder = os.environ.get('DEPTH_ANYTHING_ENCODER')
if env_encoder:
    config['depth_anything_encoder'] = env_encoder
env_fast = os.environ.get('FAST_MODE')
if env_fast:
    # fast mode: lower internal sizes/iters but keep fps the same
    config['longer_side_len'] = min(int(config.get('longer_side_len', 768)), 640)
    config['sparse_iter'] = 1
    config['largest_size'] = 320
    config['redundant_number'] = min(int(config.get('redundant_number', 12)), 6)
    config['repeat_inpaint_edge'] = False
    config['extrapolation_thickness'] = 0
    # shrink neighborhoods to reduce graph size
    config['background_thickness'] = max(20, int(config.get('background_thickness', 70) * 0.5))
    config['context_thickness'] = max(40, int(config.get('context_thickness', 140) * 0.5))

# Optional video-related overrides
env_dur = os.environ.get('DURATION_SECONDS')
env_fps = os.environ.get('FPS_OVERRIDE')
env_loop = os.environ.get('LOOP_MODE')
env_speed = os.environ.get('SPEED_MULTIPLIER')
# Enforce constant FPS at 30 regardless of UI
config['fps'] = 30
if env_dur:
    try:
        duration_seconds = max(0.5, float(env_dur))
        config['num_frames'] = int(round(config['fps'] * duration_seconds))
    except Exception:
        pass
if env_loop:
    config['loop_motion'] = True
if env_speed:
    try:
        config['speed_multiplier'] = float(env_speed)
    except Exception:
        pass

# Optional explicit pixel crop after aspect crop
def _read_px(name: str):
    val = os.environ.get(name)
    if val is None:
        return None
    try:
        return max(0, int(float(val)))
    except Exception:
        return None
crop_top_px = _read_px('CROP_TOP_PX')
crop_bottom_px = _read_px('CROP_BOTTOM_PX')
crop_left_px = _read_px('CROP_LEFT_PX')
crop_right_px = _read_px('CROP_RIGHT_PX')
if any(v is not None for v in [crop_top_px, crop_bottom_px, crop_left_px, crop_right_px]):
    config['pixel_crop'] = {
        'top': crop_top_px or 0,
        'bottom': crop_bottom_px or 0,
        'left': crop_left_px or 0,
        'right': crop_right_px or 0,
    }
else:
    # Default crop: 150px on all sides after aspect crop
    config['pixel_crop'] = {'top': 150, 'bottom': 150, 'left': 150, 'right': 150}

# Optional IO path overrides for per-job isolation
env_src = os.environ.get('SRC_FOLDER')
env_depth = os.environ.get('DEPTH_FOLDER')
env_mesh = os.environ.get('MESH_FOLDER')
env_video = os.environ.get('VIDEO_FOLDER')
if env_src:
    config['src_folder'] = env_src
if env_depth:
    config['depth_folder'] = env_depth
if env_mesh:
    config['mesh_folder'] = env_mesh
if env_video:
    config['video_folder'] = env_video

# Progress reporting
PROGRESS_FILE = os.environ.get('PROGRESS_FILE')
def write_progress(percent: int, message: str):
    if not PROGRESS_FILE:
        return
    try:
        import json
        with open(PROGRESS_FILE, 'w') as f:
            json.dump({'percent': int(percent), 'message': message}, f)
    except Exception:
        pass
if config['offscreen_rendering'] is True:
    vispy.use(app='egl')
os.makedirs(config['mesh_folder'], exist_ok=True)
os.makedirs(config['video_folder'], exist_ok=True)
os.makedirs(config['depth_folder'], exist_ok=True)
sample_list = get_MiDaS_samples(config['src_folder'], config['depth_folder'], config, config['specific'])
normal_canvas, all_canvas = None, None

if isinstance(config["gpu_ids"], int) and (config["gpu_ids"] >= 0):
    device = config["gpu_ids"]
else:
    device = "cpu"

print(f"running on device {device}")

# helper to create filesystem-safe names for debug assets
def _safe_name(x: str) -> str:
    return re.sub(r"[^A-Za-z0-9._-]+", "_", str(x))

for idx in tqdm(range(len(sample_list))):
    write_progress(1, 'Starting')
    depth = None
    sample = sample_list[idx]
    print("Current Source ==> ", sample['src_pair_name'])
    mesh_fi = os.path.join(config['mesh_folder'], sample['src_pair_name'] +'.ply')
    image = imageio.imread(sample['ref_img_fi'])

    print(f"Running depth extraction at {time.time()}")
    if config.get('use_depth_anything_v2', False):
        run_depth_anything_v2([sample['ref_img_fi']], config['depth_folder'], encoder=config.get('depth_anything_encoder', 'vits'), input_size=518)
    elif config.get('use_boostmonodepth') is True:
        # Lazy import to avoid importing optional deps when not needed
        from boostmonodepth_utils import run_boostmonodepth
        run_boostmonodepth(sample['ref_img_fi'], config['src_folder'], config['depth_folder'])
    elif config.get('require_midas') is True:
        # Lazy import MiDaS only if required
        from MiDaS.run import run_depth
        from MiDaS.monodepth_net import MonoDepthNet
        import MiDaS.MiDaS_utils as MiDaS_utils
        run_depth([sample['ref_img_fi']], config['src_folder'], config['depth_folder'],
                  config['MiDaS_model_ckpt'], MonoDepthNet, MiDaS_utils, target_w=448)
    write_progress(15, 'Depth estimated')
    # As soon as depth is estimated, export the raw depth from depth/ as a PNG for live preview
    if DEBUG_MODE and DEBUG_DIR:
        try:
            os.makedirs(DEBUG_DIR, exist_ok=True)
            raw_path = sample.get('depth_fi')
            base_dbg = _safe_name(sample.get('src_pair_name') or sample.get('tgt_name') or 'input')
            if isinstance(raw_path, str) and os.path.exists(raw_path):
                if raw_path.lower().endswith('.npy'):
                    d_raw = np.load(raw_path)
                else:
                    d_raw = imageio.imread(raw_path)
                if d_raw is not None:
                    dmin, dmax = float(np.min(d_raw)), float(np.max(d_raw))
                    dn = (d_raw - dmin) / max(1e-6, (dmax - dmin))
                    dn = (dn * 255.0).astype(np.uint8)
                    # Downscale-only to requested processing size for live preview
                    try:
                        req_long = int(config.get('longer_side_len', max(dn.shape[:2])))
                        cur_long = max(dn.shape[0], dn.shape[1])
                        eff_long = min(req_long, cur_long)
                        frac_dbg = float(eff_long) / float(cur_long) if cur_long > 0 else 1.0
                        if frac_dbg < 1.0:
                            tgt_h = max(1, int(dn.shape[0] * frac_dbg))
                            tgt_w = max(1, int(dn.shape[1] * frac_dbg))
                            dn = cv2.resize(dn, (tgt_w, tgt_h), interpolation=cv2.INTER_AREA)
                    except Exception:
                        pass
                    cv2.imwrite(os.path.join(DEBUG_DIR, f"{base_dbg}_depth_raw.png"), dn)
        except Exception:
            pass

    if 'npy' in config['depth_format']:
        config['output_h'], config['output_w'] = np.load(sample['depth_fi']).shape[:2]
    else:
        config['output_h'], config['output_w'] = imageio.imread(sample['depth_fi']).shape[:2]
    # Strictly no upscaling: cap to image longer side and also cap to requested limit, whichever is smaller
    requested_longer = int(config.get('longer_side_len', max(config['output_h'], config['output_w'])))
    current_longer = max(config['output_h'], config['output_w'])
    effective_longer = min(requested_longer, current_longer)
    frac = float(effective_longer) / float(current_longer) if current_longer > 0 else 1.0
    # Ensure even dimensions early to avoid later resizes in video writer
    new_h = int(config['output_h'] * frac); new_w = int(config['output_w'] * frac)
    new_h -= (new_h % 2); new_w -= (new_w % 2)
    config['output_h'], config['output_w'] = max(2, new_h), max(2, new_w)
    config['original_h'], config['original_w'] = config['output_h'], config['output_w']
    if image.ndim == 2:
        image = image[..., None].repeat(3, -1)
    elif image.ndim == 3 and image.shape[2] == 4:
        # Convert RGBA to RGB if image has 4 channels
        image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
    if np.sum(np.abs(image[..., 0] - image[..., 1])) == 0 and np.sum(np.abs(image[..., 1] - image[..., 2])) == 0:
        config['gray_image'] = True
    else:
        config['gray_image'] = False
    image = cv2.resize(image, (config['output_w'], config['output_h']), interpolation=cv2.INTER_AREA)
    depth = read_MiDaS_depth(sample['depth_fi'], 3.0, config['output_h'], config['output_w'])
    if DEBUG_MODE and DEBUG_DIR:
        try:
            os.makedirs(DEBUG_DIR, exist_ok=True)
            # Save resized input image used downstream (already no-upscale processed size)
            base_dbg = _safe_name(sample.get('tgt_name') or sample.get('src_pair_name') or 'input')
            cv2.imwrite(os.path.join(DEBUG_DIR, f"{base_dbg}_image.png"), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
            # Save raw depth (pre-filter) quick preview
            d0 = (depth - depth.min()) / max(1e-6, (depth.max() - depth.min()))
            cv2.imwrite(os.path.join(DEBUG_DIR, f"{base_dbg}_depth_pre.png"), (d0*255).astype(np.uint8))
        except Exception:
            pass
    mean_loc_depth = depth[depth.shape[0]//2, depth.shape[1]//2]
    if not(config['load_ply'] is True and os.path.exists(mesh_fi)):
        write_progress(25, 'Filtering depth')
        vis_photos, vis_depths = sparse_bilateral_filtering(depth.copy(), image.copy(), config, num_iter=config['sparse_iter'], spdb=False)
        depth = vis_depths[-1]
        if DEBUG_MODE and DEBUG_DIR:
            try:
                # Save depth after filtering—the one actually used by the pipeline
                d1 = (depth - depth.min()) / max(1e-6, (depth.max() - depth.min()))
                base_dbg = _safe_name(sample.get('tgt_name') or sample.get('src_pair_name') or 'input')
                # ensure debug image is saved at the current output resolution the user selected
                dbg_h, dbg_w = int(config['output_h']), int(config['output_w'])
                d1_img = (d1*255).astype(np.uint8)
                if d1_img.shape[0] != dbg_h or d1_img.shape[1] != dbg_w:
                    d1_img = cv2.resize(d1_img, (dbg_w, dbg_h), interpolation=cv2.INTER_NEAREST)
                cv2.imwrite(os.path.join(DEBUG_DIR, f"{base_dbg}_depth_post.png"), d1_img)
            except Exception:
                pass
        model = None
        torch.cuda.empty_cache()
        print("Start Running 3D_Photo ...")
        print(f"Loading edge model at {time.time()}")
        depth_edge_model = Inpaint_Edge_Net(init_weights=True)
        depth_edge_weight = torch.load(config['depth_edge_model_ckpt'],
                                       map_location=torch.device(device))
        depth_edge_model.load_state_dict(depth_edge_weight)
        depth_edge_model = depth_edge_model.to(device)
        depth_edge_model.eval()

        print(f"Loading depth model at {time.time()}")
        depth_feat_model = Inpaint_Depth_Net()
        depth_feat_weight = torch.load(config['depth_feat_model_ckpt'],
                                       map_location=torch.device(device))
        depth_feat_model.load_state_dict(depth_feat_weight, strict=True)
        depth_feat_model = depth_feat_model.to(device)
        depth_feat_model.eval()
        depth_feat_model = depth_feat_model.to(device)
        print(f"Loading rgb model at {time.time()}")
        rgb_model = Inpaint_Color_Net()
        rgb_feat_weight = torch.load(config['rgb_feat_model_ckpt'],
                                     map_location=torch.device(device))
        rgb_model.load_state_dict(rgb_feat_weight)
        rgb_model.eval()
        rgb_model = rgb_model.to(device)
        graph = None


        print(f"Writing depth ply (and basically doing everything) at {time.time()}")
        write_progress(55, 'Building mesh')
        rt_info = write_ply(image,
                              depth,
                              sample['int_mtx'],
                              mesh_fi,
                              config,
                              rgb_model,
                              depth_edge_model,
                              depth_edge_model,
                              depth_feat_model)

        if rt_info is False:
            continue
        rgb_model = None
        color_feat_model = None
        depth_edge_model = None
        depth_feat_model = None
        torch.cuda.empty_cache()
    if config['save_ply'] is True or config['load_ply'] is True:
        verts, colors, faces, Height, Width, hFov, vFov = read_ply(mesh_fi)
    else:
        verts, colors, faces, Height, Width, hFov, vFov = rt_info


    print(f"Making video at {time.time()}")
    videos_poses, video_basename = copy.deepcopy(sample['tgts_poses']), sample['tgt_name']
    # Disable extra cropping in final video canvas
    border = None
    write_progress(80, 'Rendering videos')
    normal_canvas, all_canvas = output_3d_photo(verts.copy(), colors.copy(), faces.copy(), copy.deepcopy(Height), copy.deepcopy(Width), copy.deepcopy(hFov), copy.deepcopy(vFov),
                        copy.deepcopy(sample['tgt_pose']), sample['video_postfix'], copy.deepcopy(sample['ref_pose']), copy.deepcopy(config['video_folder']),
                        image.copy(), copy.deepcopy(sample['int_mtx']), config, image,
                        videos_poses, video_basename, config.get('original_h'), config.get('original_w'), border=border, depth=depth, normal_canvas=normal_canvas, all_canvas=all_canvas,
                        mean_loc_depth=mean_loc_depth)
    write_progress(100, 'Done')
